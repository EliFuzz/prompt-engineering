"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[1103],{223:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"education/prompt-engineering/overview","title":"Overview","description":"Prompt Engineering Overview","source":"@site/docs/education/01-prompt-engineering/01-overview.mdx","sourceDirName":"education/01-prompt-engineering","slug":"/education/prompt-engineering/overview","permalink":"/vibe-labs/docs/education/prompt-engineering/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/EliFuzz/vibe-labs/docs/education/01-prompt-engineering/01-overview.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Overview","description":"Prompt Engineering Overview","hide_table_of_contents":true},"sidebar":"education","previous":{"title":"Prompt Engineering","permalink":"/vibe-labs/docs/category/prompt-engineering"},"next":{"title":"Guides","permalink":"/vibe-labs/docs/education/prompt-engineering/guides"}}');var s=t(3420),r=t(8906),o=t(7),d=t(1519);const a={title:"Overview",description:"Prompt Engineering Overview",hide_table_of_contents:!0},l="Overview",c={},h=[];function p(e){const n={code:"code",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"overview",children:"Overview"})}),"\n",(0,s.jsxs)(d.A,{queryString:"primary",children:[(0,s.jsx)(o.A,{value:"glosary",label:"Glosary",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"}),": process of carefully designing and optimizing instructions (prompts) to elicit the best possible output from generative AI models, especially Large Language Models (LLMs). By providing clear, specific, and well-structured prompts, you can guide the AI to generate relevant, accurate, and high-quality responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt"}),": input you provide to a generative AI model to request a specific output. It can be a simple question, a set of instructions, or even a creative writing example"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large Language Model (LLM)"}),": AI model designed to understand and generate human-like text. LLMs are trained on vast amounts of data and can perform tasks like translation, summarization, and even creative writing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Template"}),": a pre-defined structure or format for a prompt that can be customized with specific details or variables to generate dynamic prompts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Tuning"}),": process of fine-tuning pre-trained LLMs by adapting them to specific tasks or domains through prompt engineering, rather than traditional fine-tuning methods"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Injection"}),": a security vulnerability where an attacker manipulates the input prompt to influence the AI model's behavior in unintended ways, potentially leading to unauthorized actions or disclosures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Leakage"}),": situation where sensitive information from the prompt is inadvertently included in the generated output, posing privacy or security risks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Bias"}),": tendency of an AI model to generate responses that reflect the biases present in its training data, leading to unfair or inaccurate outcomes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Hallucination"}),": when an AI model generates information that is not supported by the input prompt or its training data, leading to false or misleading outputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Testing"}),": process of evaluating and validating prompts to ensure they produce the desired output, meet quality standards, and comply with ethical and regulatory requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Optimization"}),": continuous process of refining prompts to improve their performance, based on feedback, testing results, and changes in the AI model or its training data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Window"}),": max number of tokens the model can process at once, including input and output. Often a model-specific architectural limit"]}),"\n"]})}),(0,s.jsx)(o.A,{value:"llm-settings",label:"LLM Settings",children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Category"}),(0,s.jsx)("th",{children:"Setting Parameter"}),(0,s.jsx)("th",{children:"Description"}),(0,s.jsx)("th",{children:"Low Value Use Cases"}),(0,s.jsx)("th",{children:"High Value Use Cases"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{rowspan:"3",children:"Sampling"}),(0,s.jsx)("td",{children:"Temperature"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:'controls the randomness or "creativity" of the output. Higher values\nlead to more diverse and imaginative responses, while lower values make\nthe output more deterministic and focused'})}),(0,s.jsx)("td",{children:"factual Q&A, summarization"}),(0,s.jsx)("td",{children:"story generation, poetry, brainstorming"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Top-P (Nucleus Sampling)"}),(0,s.jsx)("td",{children:(0,s.jsxs)(n.p,{children:["selects tokens from the smallest possible set whose cumulative\nprobability exceeds the ",(0,s.jsx)(n.code,{children:"top_p"})," threshold. Works in conjunction with\ntemperature to control diversity"]})}),(0,s.jsx)("td",{children:"precise answers"}),(0,s.jsx)("td",{children:"varied and imaginative text"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Top-K Sampling"}),(0,s.jsx)("td",{children:(0,s.jsxs)(n.p,{children:["limits the token selection to the top ",(0,s.jsx)(n.code,{children:"k"})," most probable tokens at each\nstep. The model will only consider words within this ",(0,s.jsx)(n.code,{children:"k"})," set. Often used\nin conjunction with Top-P"]})}),(0,s.jsx)("td",{children:(0,s.jsxs)(n.p,{children:["limits token selection to the top ",(0,s.jsx)(n.code,{children:"k"})," options for more focused output"]})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"expands token options for greater diversity and creativity, but may\ninclude less relevant choices"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Advanced Sampling"}),(0,s.jsx)("td",{children:"Logit Bias"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"allows you to modify the probability of specific tokens appearing or not\nappearing in the generated output. You can increase or decrease the\nlikelihood of certain words"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"reduces the likelihood of tokens with negative bias, prompting the model\nto avoid specific words"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"increases the likelihood of tokens with positive bias, encouraging the\nmodel to include specific words or phrases"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{rowspan:"3",children:"Output Control"}),(0,s.jsx)("td",{children:"Max Length / Max Tokens"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"sets the maximum number of tokens the model will generate in its\nresponse. This includes both the input prompt and the generated output\nin some APIs"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"summarization, quick answers: concise, cost-effective responses, cutting\noff if necessary"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"essay generation, code generation, detailed explanations: more detailed\nresponses, but manage to avoid irrelevance and high costs"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Stop Sequences"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"string or list of strings that, when encountered in the generated\noutput, will stop the model from generating further tokens"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"stops generating text at specified sequences, ensuring structured\noutputs and preventing run-ons"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"continues generating until reaching max tokens or an end-of-text token"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"N (Number of Completions)"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"specifies how many independent completions (responses) the model should\ngenerate for a single prompt"})}),(0,s.jsx)("td",{children:"produces one response, typical for direct answers"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"creates several distinct responses for selection or variation,\npotentially increasing cost"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Repetition Control"}),(0,s.jsx)("td",{children:"Frequency Penalty"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"applies a penalty to new tokens based on how many times that token has\nalready appeared in the text (prompt + generated response)"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"allows repetition with less penalty, increasing the likelihood of\nrepeated words or phrases"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"imposes a higher penalty on repetition, promoting new vocabulary and\ndiscouraging repeated tokens"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Repetition Control"}),(0,s.jsx)("td",{children:"Presence Penalty"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"imposes a uniform penalty on new tokens that have appeared in the text\nat least once, regardless of their frequency"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"reduces penalties on previously mentioned tokens to maintain focus on a\nspecific topic"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"increases penalties on previously used tokens to encourage diverse and\ndistinct ideas"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Reproducibility"}),(0,s.jsx)("td",{children:"Seed"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"setting a seed makes the model's output deterministic for a given set of\nparameters"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"guarantees consistent results for repeated calls with the same prompt\nand settings, aiding debugging and reproducibility"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"each call with the same prompt and settings yields a different output,\nwhile still adhering to other parameters"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Input Processing"}),(0,s.jsx)("td",{children:"Context Window (Max Context Length)"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"maximum number of tokens (input prompt + generated output) that the\nmodel can process and consider at one time. This is often a\nmodel-specific architectural limit"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"short prompts limit the model's memory of prior conversation, causing\ncontext loss in longer interactions"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"long conversations and large document analysis allow the model to\nmaintain context, enhancing coherence and relevance in extended\ninteractions"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Model Selection"}),(0,s.jsx)("td",{children:"Model Name/ID"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"specifies the particular LLM variant or version to be used. Different\nmodels have varying capabilities, sizes, and training data"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"smaller models may produce lower quality, less nuanced responses and\nhave limited capabilities"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"larger models generally provide higher quality, more nuanced responses,\nbut may incur higher costs and slower inference"})})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Generation Strategy"}),(0,s.jsx)("td",{children:"Decoding Type"}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:"refers to the algorithm used to select the next token. Common types\ninclude greedy decoding, beam search, and sampling (which involves\ntemperature, top-p, top-k)"})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:'"Greedy" selection yields deterministic but potentially less creative\noutput by always choosing the highest probability'})}),(0,s.jsx)("td",{children:(0,s.jsx)(n.p,{children:'"Sampling" adds variability, while "beam search" explores multiple\nsequences to identify more globally optimal outputs'})})]})]})]})})]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}}}]);