"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[1103],{7:(e,t,n)=>{n.d(t,{A:()=>o});n(6672);var r=n(3526);const i={tabItem:"tabItem__pw4"};var s=n(3420);function o({children:e,hidden:t,className:n}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,r.A)(i.tabItem,n),hidden:t,children:e})}},223:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>p,frontMatter:()=>l,metadata:()=>r,toc:()=>u});const r=JSON.parse('{"id":"education/prompt-engineering/overview","title":"Overview","description":"Prompt Engineering Overview","source":"@site/docs/education/01-prompt-engineering/01-overview.mdx","sourceDirName":"education/01-prompt-engineering","slug":"/education/prompt-engineering/overview","permalink":"/vibe-labs/docs/education/prompt-engineering/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/EliFuzz/vibe-labs/docs/education/01-prompt-engineering/01-overview.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Overview","description":"Prompt Engineering Overview","hide_table_of_contents":true},"sidebar":"education","previous":{"title":"Prompt Engineering","permalink":"/vibe-labs/docs/category/prompt-engineering"},"next":{"title":"Guides","permalink":"/vibe-labs/docs/education/prompt-engineering/guides"}}');var i=n(3420),s=n(8906),o=n(7),a=n(1519);const l={title:"Overview",description:"Prompt Engineering Overview",hide_table_of_contents:!0},d="Overview",c={},u=[];function h(e){const t={code:"code",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"overview",children:"Overview"})}),"\n",(0,i.jsxs)(a.A,{queryString:"primary",children:[(0,i.jsx)(o.A,{value:"glosary",label:"Glosary",children:(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Engineering"}),": process of carefully designing and optimizing instructions (prompts) to elicit the best possible output from generative AI models, especially Large Language Models (LLMs). By providing clear, specific, and well-structured prompts, you can guide the AI to generate relevant, accurate, and high-quality responses"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt"}),": input you provide to a generative AI model to request a specific output. It can be a simple question, a set of instructions, or even a creative writing example"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Large Language Model (LLM)"}),": AI model designed to understand and generate human-like text. LLMs are trained on vast amounts of data and can perform tasks like translation, summarization, and even creative writing"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Template"}),": a pre-defined structure or format for a prompt that can be customized with specific details or variables to generate dynamic prompts"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Tuning"}),": process of fine-tuning pre-trained LLMs by adapting them to specific tasks or domains through prompt engineering, rather than traditional fine-tuning methods"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Injection"}),": a security vulnerability where an attacker manipulates the input prompt to influence the AI model's behavior in unintended ways, potentially leading to unauthorized actions or disclosures"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Leakage"}),": situation where sensitive information from the prompt is inadvertently included in the generated output, posing privacy or security risks"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Bias"}),": tendency of an AI model to generate responses that reflect the biases present in its training data, leading to unfair or inaccurate outcomes"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Hallucination"}),": when an AI model generates information that is not supported by the input prompt or its training data, leading to false or misleading outputs"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Testing"}),": process of evaluating and validating prompts to ensure they produce the desired output, meet quality standards, and comply with ethical and regulatory requirements"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prompt Optimization"}),": continuous process of refining prompts to improve their performance, based on feedback, testing results, and changes in the AI model or its training data"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Context Window"}),": max number of tokens the model can process at once, including input and output. Often a model-specific architectural limit"]}),"\n"]})}),(0,i.jsx)(o.A,{value:"llm-settings",label:"LLM Settings",children:(0,i.jsxs)("table",{children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("th",{children:"Category"}),(0,i.jsx)("th",{children:"Setting Parameter"}),(0,i.jsx)("th",{children:"Description"}),(0,i.jsx)("th",{children:"Low Value Use Cases"}),(0,i.jsx)("th",{children:"High Value Use Cases"})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{rowspan:"3",children:"Sampling"}),(0,i.jsx)("td",{children:"Temperature"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:'controls the randomness or "creativity" of the output. Higher values\nlead to more diverse and imaginative responses, while lower values make\nthe output more deterministic and focused'})}),(0,i.jsx)("td",{children:"factual Q&A, summarization"}),(0,i.jsx)("td",{children:"story generation, poetry, brainstorming"})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Top-P (Nucleus Sampling)"}),(0,i.jsx)("td",{children:(0,i.jsxs)(t.p,{children:["selects tokens from the smallest possible set whose cumulative\nprobability exceeds the ",(0,i.jsx)(t.code,{children:"top_p"})," threshold. Works in conjunction with\ntemperature to control diversity"]})}),(0,i.jsx)("td",{children:"precise answers"}),(0,i.jsx)("td",{children:"varied and imaginative text"})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Top-K Sampling"}),(0,i.jsx)("td",{children:(0,i.jsxs)(t.p,{children:["limits the token selection to the top ",(0,i.jsx)(t.code,{children:"k"})," most probable tokens at each\nstep. The model will only consider words within this ",(0,i.jsx)(t.code,{children:"k"})," set. Often used\nin conjunction with Top-P"]})}),(0,i.jsx)("td",{children:(0,i.jsxs)(t.p,{children:["limits token selection to the top ",(0,i.jsx)(t.code,{children:"k"})," options for more focused output"]})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"expands token options for greater diversity and creativity, but may\ninclude less relevant choices"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Advanced Sampling"}),(0,i.jsx)("td",{children:"Logit Bias"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"allows you to modify the probability of specific tokens appearing or not\nappearing in the generated output. You can increase or decrease the\nlikelihood of certain words"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"reduces the likelihood of tokens with negative bias, prompting the model\nto avoid specific words"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"increases the likelihood of tokens with positive bias, encouraging the\nmodel to include specific words or phrases"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{rowspan:"3",children:"Output Control"}),(0,i.jsx)("td",{children:"Max Length / Max Tokens"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"sets the maximum number of tokens the model will generate in its\nresponse. This includes both the input prompt and the generated output\nin some APIs"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"summarization, quick answers: concise, cost-effective responses, cutting\noff if necessary"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"essay generation, code generation, detailed explanations: more detailed\nresponses, but manage to avoid irrelevance and high costs"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Stop Sequences"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"string or list of strings that, when encountered in the generated\noutput, will stop the model from generating further tokens"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"stops generating text at specified sequences, ensuring structured\noutputs and preventing run-ons"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"continues generating until reaching max tokens or an end-of-text token"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"N (Number of Completions)"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"specifies how many independent completions (responses) the model should\ngenerate for a single prompt"})}),(0,i.jsx)("td",{children:"produces one response, typical for direct answers"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"creates several distinct responses for selection or variation,\npotentially increasing cost"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Repetition Control"}),(0,i.jsx)("td",{children:"Frequency Penalty"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"applies a penalty to new tokens based on how many times that token has\nalready appeared in the text (prompt + generated response)"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"allows repetition with less penalty, increasing the likelihood of\nrepeated words or phrases"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"imposes a higher penalty on repetition, promoting new vocabulary and\ndiscouraging repeated tokens"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Repetition Control"}),(0,i.jsx)("td",{children:"Presence Penalty"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"imposes a uniform penalty on new tokens that have appeared in the text\nat least once, regardless of their frequency"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"reduces penalties on previously mentioned tokens to maintain focus on a\nspecific topic"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"increases penalties on previously used tokens to encourage diverse and\ndistinct ideas"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Reproducibility"}),(0,i.jsx)("td",{children:"Seed"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"setting a seed makes the model's output deterministic for a given set of\nparameters"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"guarantees consistent results for repeated calls with the same prompt\nand settings, aiding debugging and reproducibility"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"each call with the same prompt and settings yields a different output,\nwhile still adhering to other parameters"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Input Processing"}),(0,i.jsx)("td",{children:"Context Window (Max Context Length)"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"maximum number of tokens (input prompt + generated output) that the\nmodel can process and consider at one time. This is often a\nmodel-specific architectural limit"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"short prompts limit the model's memory of prior conversation, causing\ncontext loss in longer interactions"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"long conversations and large document analysis allow the model to\nmaintain context, enhancing coherence and relevance in extended\ninteractions"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Model Selection"}),(0,i.jsx)("td",{children:"Model Name/ID"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"specifies the particular LLM variant or version to be used. Different\nmodels have varying capabilities, sizes, and training data"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"smaller models may produce lower quality, less nuanced responses and\nhave limited capabilities"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"larger models generally provide higher quality, more nuanced responses,\nbut may incur higher costs and slower inference"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Generation Strategy"}),(0,i.jsx)("td",{children:"Decoding Type"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"refers to the algorithm used to select the next token. Common types\ninclude greedy decoding, beam search, and sampling (which involves\ntemperature, top-p, top-k)"})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:'"Greedy" selection yields deterministic but potentially less creative\noutput by always choosing the highest probability'})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:'"Sampling" adds variability, while "beam search" explores multiple\nsequences to identify more globally optimal outputs'})})]})]})]})})]})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},1519:(e,t,n)=>{n.d(t,{A:()=>w});var r=n(6672),i=n(3526),s=n(880),o=n(5291),a=n(7387),l=n(1981),d=n(2962),c=n(621);function u(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??function(e){return u(e).map((({props:{value:e,label:t,attributes:n,default:r}})=>({value:e,label:t,attributes:n,default:r})))}(n);return function(e){const t=(0,d.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function p({value:e,tabValues:t}){return t.some((t=>t.value===e))}function m({queryString:e=!1,groupId:t}){const n=(0,o.W6)(),i=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,l.aZ)(i),(0,r.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(n.location.search);t.set(i,e),n.replace({...n.location,search:t.toString()})}),[i,n])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:i}=e,s=h(e),[o,l]=(0,r.useState)((()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find((e=>e.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:s}))),[d,u]=m({queryString:n,groupId:i}),[g,x]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,i]=(0,c.Dv)(t);return[n,(0,r.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:i}),j=(()=>{const e=d??g;return p({value:e,tabValues:s})?e:null})();(0,a.A)((()=>{j&&l(j)}),[j]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!p({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),x(e)}),[u,x,s]),tabValues:s}}var x=n(2521);const j={tabList:"tabList_MPh5",tabItem:"tabItem_WAIp"};var f=n(3420);function v({className:e,block:t,selectedValue:n,selectValue:r,tabValues:o}){const a=[],{blockElementScrollPositionUntilNextRender:l}=(0,s.a_)(),d=e=>{const t=e.currentTarget,i=a.indexOf(t),s=o[i].value;s!==n&&(l(t),r(s))},c=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=a.indexOf(e.currentTarget)+1;t=a[n]??a[0];break}case"ArrowLeft":{const n=a.indexOf(e.currentTarget)-1;t=a[n]??a[a.length-1];break}}t?.focus()};return(0,f.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":t},e),children:o.map((({value:e,label:t,attributes:r})=>(0,f.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{a.push(e)},onKeyDown:c,onClick:d,...r,className:(0,i.A)("tabs__item",j.tabItem,r?.className,{"tabs__item--active":n===e}),children:t??e},e)))})}function b({lazy:e,children:t,selectedValue:n}){const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=s.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,f.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n})))})}function y(e){const t=g(e);return(0,f.jsxs)("div",{className:(0,i.A)("tabs-container",j.tabList),children:[(0,f.jsx)(v,{...t,...e}),(0,f.jsx)(b,{...t,...e})]})}function w(e){const t=(0,x.A)();return(0,f.jsx)(y,{...e,children:u(e.children)},String(t))}},8906:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var r=n(6672);const i={},s=r.createContext(i);function o(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);