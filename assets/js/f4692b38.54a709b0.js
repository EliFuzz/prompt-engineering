"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[6403],{6448:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"education/prompt-engineering/techniques","title":"Techniques","description":"Prompt Techniques","source":"@site/docs/education/01-prompt-engineering/03-techniques.mdx","sourceDirName":"education/01-prompt-engineering","slug":"/education/prompt-engineering/techniques","permalink":"/vibe-labs/docs/education/prompt-engineering/techniques","draft":false,"unlisted":false,"editUrl":"https://github.com/EliFuzz/vibe-labs/docs/education/01-prompt-engineering/03-techniques.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Techniques","description":"Prompt Techniques","hide_table_of_contents":true},"sidebar":"education","previous":{"title":"Guides","permalink":"/vibe-labs/docs/education/prompt-engineering/guides"},"next":{"title":"Risks","permalink":"/vibe-labs/docs/education/prompt-engineering/risks"}}');var r=t(3420),s=t(8906);const a={title:"Techniques",description:"Prompt Techniques",hide_table_of_contents:!0},o="Prompt Techniques",l={},c=[];function d(e){const n={h1:"h1",header:"header",p:"p",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"prompt-techniques",children:"Prompt Techniques"})}),"\n",(0,r.jsxs)("table",{children:[(0,r.jsx)("thead",{children:(0,r.jsxs)("tr",{children:[(0,r.jsx)("th",{children:"Technique"}),(0,r.jsx)("th",{children:"Description"}),(0,r.jsx)("th",{children:"Mechanism"}),(0,r.jsx)("th",{children:"Benefits"}),(0,r.jsx)("th",{children:"Limitations"}),(0,r.jsx)("th",{children:"Use Cases"})]})}),(0,r.jsxs)("tbody",{children:[(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Zero-shot Prompting"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"LLM directly prompted to perform a task without any prior specific\nexamples. It relies on the LLM's pre-trained knowledge to infer an\nappropriate response"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The LLM leverages its extensive pre-trained knowledge base to understand\nthe given prompt and generate a relevant response without seeing any\ntask-specific examples. It's about direct task instruction"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Simplicity, works out-of-the-box for many tasks, ideal for quick queries"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Can struggle with complex or novel tasks, may produce less accurate or\ninconsistent results compared to other methods requiring examples"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Question answering, summarization, simple text generation, language\ntranslation"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Few-shot Prompting"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"LLM is provided with a small number of input-output examples (typically\n1-5) of a task within the prompt to guide its understanding and\ngeneration of the desired output"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The LLM observes the provided examples, learns the pattern and desired\nbehavior, and then applies this learned context to solve a new, unseen\ninput of the same task. It's in-context learning"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Significantly improves performance on various tasks, especially for\nspecific formats or styles; reduces the need for extensive fine-tuning;\nadaptable to new tasks quickly"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Performance is highly dependent on the quality and diversity of the\nprovided examples; prompt length can become an issue with too many\nexamples; still less robust than fine-tuning for highly specialized\ntasks"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Sentiment analysis, classification, question answering with specific\nformats, code generation, creative writing"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Chain-of-Thought (CoT) Prompting"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Encourages LLMs to break down complex problems into a series of\nintermediate reasoning steps before arriving at a final answer. This\nmimics human-like thinking"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'The prompt explicitly instructs the LLM to "think step by step" or\nprovides examples of step-by-step reasoning. The LLM generates these\nintermediate thoughts, which then guide its subsequent reasoning and\nfinal answer generation'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Improves accuracy on complex reasoning tasks (e.g., math word problems,\ncommon sense reasoning); provides transparency into the model's thought\nprocess; enables the model to allocate more compute to problems\nrequiring more reasoning"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Can be slower due to multiple reasoning steps; works best with larger\nmodels; the generated reasoning might not always reflect the true\ninternal computation; can overcomplicate simple questions"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Mathematical reasoning, symbolic manipulation, complex question\nanswering, common sense reasoning, programming"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Meta Prompting"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"LLM is prompted to generate or refine its own prompts to better perform\na given task. It's about using the LLM to optimize its own instructions"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The user asks the LLM to create or modify a prompt for a specific goal.\nThrough iterative dialogue, the LLM refines the prompt based on user\nfeedback, aiming to produce a more effective instruction for itself or\nanother LLM"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Improves prompt quality and effectiveness; reduces manual prompt\nengineering effort; allows for dynamic prompt adaptation; can save time\nin finding optimal prompts"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires a good initial understanding of the desired outcome; can be\ncomputationally intensive if many iterations are needed; the quality of\nmeta-prompts depends on the LLM's ability to self-critique"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Prompt optimization, creating tailored instructions for specific tasks,\nrefining conversational flows, content generation for specific\ntones/styles"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Self-Consistency"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Enhances the reliability and accuracy of LLM outputs by generating\nmultiple diverse reasoning paths or responses to a given prompt and then\nselecting the most consistent or frequently occurring answer among them"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The LLM is prompted multiple times, often with CoT, to generate diverse\noutputs. For quantitative answers, a majority vote selects the most\ncommon one. For qualitative answers, the LLM itself or an external\nevaluator determines the most coherent or consistent response"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Improves accuracy, especially for tasks requiring reasoning or\ninterpretation (e.g., math, code generation); particularly useful for\nfree-form text generation where exact answers are not expected; reduces\nhallucination"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'Can be computationally more expensive due to multiple generations; may\nnot always converge to a single "best" answer for highly subjective\ntasks; requires a mechanism to evaluate consistency'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Mathematical problem solving, code generation, summarization, open-ended\nquestion answering, creative text generation"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Generate Knowledge Prompting"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"LLM is prompted to first generate relevant knowledge or facts pertinent\nto a query, and then use that generated knowledge to formulate its final\nanswer"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The prompt is structured in two stages: first, an instruction to\ngenerate background information, facts, or concepts related to the\nquery; second, an instruction to answer the original query using the\nnewly generated knowledge"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'Provides the LLM with a "self-curated" knowledge base for the specific\nquery, potentially leading to more accurate and grounded responses;\nreduces reliance on only the model\'s internal parameters for factual\naccuracy; can mitigate hallucination'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Adds an extra step to the reasoning process, increasing latency; quality\nof the final answer depends on the accuracy and relevance of the\ngenerated knowledge; can still generate incorrect knowledge"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Factual question answering, research assistance, content creation\nrequiring specific factual grounding"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Prompt Chaining"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Involves breaking down a complex task into a series of smaller,\nsequential prompts, where the output of one prompt serves as the input\nor context for the next"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"A sequence of prompts is defined, each designed to perform a specific\nsub-task. The LLM processes the first prompt, its output is captured,\nand then concatenated with the next prompt in the chain, continuing\nuntil the final desired output is achieved"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Enables the handling of multi-step complex tasks; ensures coherence and\nconsistency across generated text; provides more control over the\ngeneration process; can be used to build sophisticated conversational\nagents"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires careful design of each prompt in the chain; errors in earlier\nprompts can propagate; debugging can be challenging; can be\nresource-intensive for very long chains"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Multi-turn conversations, data extraction pipelines, structured content\ngeneration (e.g., reports, articles with specific sections), complex\nproblem-solving workflows"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Tree of Thoughts (ToT)"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"An advanced reasoning framework that generalizes Chain-of-Thought by\nexploring multiple reasoning paths in a tree-like structure, allowing\nfor backtracking and exploration of alternative solutions"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'Instead of a single linear chain, the LLM generates multiple "thoughts"\n(intermediate reasoning steps) at each stage. These thoughts form\nbranches, and the system can evaluate their viability, prune unpromising\npaths, and backtrack to explore others, often using a search algorithm\n(e.g., Breadth-First Search, Depth-First Search)'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Significantly improves performance on tasks requiring deeper, strategic\nthinking and decision-making; allows for more robust exploration of\nproblem spaces; can mitigate early errors by exploring alternative paths"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Computationally intensive due to the exploration of multiple paths;\ncomplex to implement and fine-tune; requires careful design of\nevaluation functions for pruning and path selection"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Planning, complex problem-solving (e.g., game playing, logistics),\ncreative generation with constraints, code debugging"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Retrieval Augmented Generation (RAG)"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"A framework that combines the generative capabilities of LLMs with\nexternal information retrieval systems (e.g., databases, search engines)\nto ground the LLM's responses in factual, up-to-date knowledge"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"When a query is received, a retrieval component searches an external\nknowledge base for relevant documents or information. This retrieved\ninformation is then provided as context to the LLM, which uses both its\ninternal knowledge and the external data to generate a more accurate and\ninformed response"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Addresses LLM limitations like hallucination and outdated information;\nprovides access to fresh, domain-specific, or proprietary data; enhances\nfactual accuracy and trustworthiness of outputs; reduces the need for\nconstant model retraining"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires an efficient and relevant retrieval system; can be complex to\nset up and maintain the knowledge base; potential for noise if retrieved\ninformation is irrelevant or low quality; latency introduced by the\nretrieval step"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Factual question answering, enterprise chatbots, knowledge management,\nsummarization of specific documents, legal research"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Automatic Reasoning and Tool-use (ART)"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"A framework that enables LLMs to automatically break down problems,\nreason through steps, and dynamically use external tools (e.g.,\ncalculators, search engines, code interpreters) to solve complex tasks"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"ART maintains a task library of example problems and their step-by-step\nsolutions, and a tool library. When facing a new problem, it finds\nsimilar examples, guides the LLM to generate a step-by-step solution,\nand automatically pauses for tool usage, coordinating between the LLM's\nthought process and external tools"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Enhances LLM capabilities beyond pure language generation; improves\naccuracy on multi-step reasoning problems; reduces reliance on manual\nprompt engineering for tool integration; increases problem-solving\nflexibility"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Can suffer from cascading errors if early steps are incorrect;\nperformance is limited by the quality of generated code or tool usage;\ntask selection from the library isn't always perfect; integration with\nvarious tools can be complex"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Scientific problem solving, complex data analysis, interactive agents,\ntechnical support, educational tutoring systems"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Automatic Prompt Engineer (APE)"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"An innovative solution where an AI system autonomously generates,\noptimizes, and selects prompts to achieve desired LLM outputs,\nsignificantly reducing manual effort"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"APE typically receives input-output pairs as examples. It then uses\ntechniques like reinforcement learning, gradient-based optimization, or\nmeta-prompting to generate and evaluate candidate prompts. The process\niterates, refining prompts based on feedback on how well the generated\nresponses match the expected outputs"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Dramatically reduces the time and effort required for prompt\nengineering; can discover highly effective prompts that humans might\nmiss; improves consistency and quality of LLM outputs across various\napplications"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Can be computationally expensive during the optimization process;\nrequires significant labeled data for training the APE system;\ninterpretability of automatically generated prompts can be low"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Optimizing prompts for chatbots, content generation, data extraction,\nfine-tuning LLM behavior for specific tasks"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Active-Prompt"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Improves Chain-of-Thought (CoT) prompting performance by selectively\nhuman-annotating exemplars (examples) where the model shows the most\nuncertainty"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'Instead of uniformly sampling examples, Active-Prompt identifies\ninstances where the LLM is most uncertain about its reasoning path.\nThese "uncertain" instances are then prioritized for human annotation to\ncreate high-quality CoT exemplars, which are then used to improve the\nLLM\'s performance'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Focuses human annotation effort on the most impactful examples;\npotentially achieves better performance with less human labeling work;\nimproves the robustness of CoT reasoning"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires a mechanism to quantify LLM uncertainty; involves\nhuman-in-the-loop for selective annotation; can be more complex to\nimplement than standard CoT"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Improving CoT for specific domains or challenging tasks where the model\nfrequently errs; active learning scenarios for prompt engineering"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Directional Stimulus Prompting (DSP)"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'A framework for guiding black-box LLMs towards specific desired outputs\nby introducing subtle, instance-specific "directional stimulus" (hints\nor cues) generated by a smaller, tunable policy model'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"A small policy model (e.g., a fine-tuned T5) generates an auxiliary\nstimulus prompt for each input. This stimulus acts as a hint, subtly\nguiding the larger, black-box LLM (e.g., GPT-3, GPT-4) towards a desired\noutcome, without directly adjusting the LLM's parameters. The policy\nmodel is optimized via supervised fine-tuning and reinforcement learning"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Provides fine-grained, query-specific guidance for black-box LLMs;\nbypasses the need for direct LLM fine-tuning; allows for more controlled\noutput generation (e.g., including specific keywords); can enhance\nreasoning accuracy"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Relies on the effectiveness of the smaller policy model; can still guide\nthe LLM towards biased or harmful content if the policy model is\noptimized incorrectly; adds an extra layer of complexity"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Summarization with specific keyword requirements, dialogue response\ngeneration, controlled text generation, guiding LLMs for specific\nstylistic outputs"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Program-Aided Language Models (PAL)"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"A novel approach that combines LLMs with external interpreters (e.g.,\nPython) to improve their reasoning capabilities, particularly for\nmathematical, logical, and algorithmic problems"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The LLM interprets the natural language prompt and, during its reasoning\nstep, generates a program (e.g., Python code) that encapsulates the\nlogic required to solve the problem. This generated program is then\nexecuted by an external interpreter, and the result is used to formulate\nthe final answer"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Significantly improves accuracy on tasks requiring precise computation\nor symbolic manipulation; offloads complex calculations to reliable\nexternal tools; allows LLMs to leverage programming logic beyond their\ninternal neural network capabilities"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires the LLM to have sufficient coding ability; introduces a\ndependency on external interpreters; debugging errors in generated code\ncan be challenging; execution time includes code generation and\ninterpretation"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Mathematical problem solving, data processing, logical puzzles,\nalgorithmic tasks, generating and executing SQL queries"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"ReAct"}),(0,r.jsx)("td",{children:(0,r.jsxs)(n.p,{children:["A prompting strategy that interweaves ",(0,r.jsx)("code",{children:"Thought"})," (reasoning)\nand ",(0,r.jsx)("code",{children:"Action"})," (tool-use) steps, allowing LLMs to dynamically\nreason and interact with external environments to gather information or\nperform operations"]})}),(0,r.jsx)("td",{children:(0,r.jsxs)(n.p,{children:["The LLM generates a ",(0,r.jsx)("code",{children:"Thought"})," which explains its reasoning\nprocess, then an ",(0,r.jsx)("code",{children:"Action"})," which specifies a tool to use\n(e.g., search engine, calculator) and its arguments. The tool's output\nis observed, and the LLM then generates another ",(0,r.jsx)("code",{children:"Thought"})," and"," ","\n",(0,r.jsx)("code",{children:"Action"})," based on the new information, iterating until the\ntask is complete"]})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Enables LLMs to perform multi-step tasks requiring external knowledge or\ncomputation; provides a transparent trace of reasoning and actions;\nallows for dynamic planning and adaptation to environmental feedback;\nreduces hallucinations by grounding responses in real-world data"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Can be complex to design effective action spaces and tool integrations;\nperformance heavily relies on the quality of tools and the LLM's ability\nto interpret tool outputs; potential for infinite loops or incorrect\naction sequences"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Complex question answering requiring external data, interactive problem\nsolving, web Browse, task automation, scientific experimentation"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Reflexion"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"A general prompting strategy that involves having LLMs analyze their own\noutputs, behaviors, knowledge, or reasoning processes to identify errors\nand iteratively improve their performance"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'The LLM generates an initial output. A "reflection" module (which can be\nanother LLM or a set of rules) then critically assesses this output,\nproviding feedback, critique, or recommendations for improvement. The\noriginal LLM then uses this feedback to generate a refined output,\nrepeating the process until a satisfactory result is achieved'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Enables self-correction and continuous improvement without human\nintervention; enhances robustness and reliability of LLM outputs;\nprovides a mechanism for learning from past mistakes; useful for tasks\nrequiring high accuracy"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:'Can be computationally intensive due to iterative refinement; the\nquality of reflection heavily depends on the reflection mechanism;\npotential for "reflection loops" if the model struggles to self-correct;\nmay not always converge to the optimal solution'})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Code debugging and improvement, content refinement, factual correction,\ncomplex task completion where iterative improvement is beneficial"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Multimodal CoT"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Extends the Chain-of-Thought reasoning to incorporate multiple\nmodalities, typically language (text) and vision (images), allowing LLMs\nto reason over information presented in different forms"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"In a two-stage framework, the LLM first generates intermediate reasoning\nchains (rationales) based on information from both text and image\ninputs. Then, in the second stage, it uses these multimodal rationales\nto infer the final answer"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Enables LLMs to solve problems that require understanding and\nintegrating information from diverse modalities; improves performance on\nmultimodal reasoning tasks; can mitigate hallucination by grounding\nreasoning in visual evidence"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires multimodal LLMs capable of processing and integrating different\ndata types; complexity in aligning information across modalities; the\nquality of rationales depends on the integration of multimodal inputs"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Visual question answering (VQA), science questions with diagrams, image\ncaptioning with reasoning, medical diagnosis from reports and scans"})})]}),(0,r.jsxs)("tr",{children:[(0,r.jsx)("td",{children:"Graph Prompting"}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Prompts LLMs by treating knowledge or information as a structured graph\n(e.g., knowledge graphs), allowing the LLM to leverage relational\ninformation"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"The knowledge is preprocessed and presented to the LLM as a structured\ngraph or a description derived from it, highlighting entities and their\nrelationships. The prompt guides the LLM to reason over this graph\nstructure to answer queries or generate text"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Leverages the rich relational information present in knowledge graphs;\nimproves factual consistency and reasoning over structured data; can\nenhance understanding of complex relationships between entities; allows\nfor more precise and grounded responses"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Requires the existence or construction of a knowledge graph; converting\ninformation into a graph structure can be complex; the LLM needs to be\nadept at interpreting graph representations"})}),(0,r.jsx)("td",{children:(0,r.jsx)(n.p,{children:"Question answering over knowledge graphs, fact extraction, entity\nlinking, semantic search, reasoning in complex domains (e.g.,\nbiomedical, legal)"})})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);